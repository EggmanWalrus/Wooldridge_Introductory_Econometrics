---
title: "Part One"
author: "Yanwen Wang"
date: "9/29/2021"
output: 
  html_document:
    toc: true 
    toc_float: true
    toc_collapsed: true
    number_sections: true
    theme: united
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r library and settings, message=FALSE, warning=FALSE}
library(wooldridge)
library(tidyverse)
library(scatterplot3d)
library(jtools)
library(moments)
library(car)
library(lm.beta)
library(QuantPsyc)

#Set ggplot graph theme
theme_set(theme_classic())
#Set digits number of regression outputs
set_summ_defaults(digits = 4)
```


# Chapter 1: The Nature of Econometrics Data
## Exercises
### C1

```{r 1.C1}
data(wage1)

#1
summary(wage1$educ)

#2
mean(wage1$wage)

#3
#CPI is 56.93 in 1976 and 218.08 in 2010.

#4
mean(218.08 / 56.93 * wage1$wage)

#5
table(wage1$female)
```

### C2

```{r 1.C2}
data("bwght")

#1
table(bwght$male)
filter(bwght, (cigs > 0) & (male == 0)) %>% nrow()

#2
mean(bwght$cigs)

#3
filter(bwght, (cigs > 0) & (male == 0))$cigs %>% mean()

#4
mean(bwght$fatheduc, na.rm = TRUE)

#5
mean(bwght$faminc * 1000)
sd(bwght$faminc * 1000)
```

### C3

```{r 1.C3}
data("meap01")

#1
range(meap01$math4)

#2
nrow(filter(meap01, math4 == 100))
nrow(filter(meap01, math4 == 100)) / nrow(meap01)

#3
nrow(filter(meap01, math4 == 50))

#4
mean(meap01$math4)
mean(meap01$read4)

#5
cor(meap01$math4, meap01$read4)

#6
mean(meap01$exppp)
sd(meap01$exppp)

#7
(6000 - 5500) / 5500
log(6000) - log(5000)
```

### C4

```{r 1.C4}
data("jtrain2")

#1
length(jtrain2$train[jtrain2$train==1]) / length(jtrain2$train)

#2
mean(jtrain2$re78[jtrain2$train==1])
mean(jtrain2$re78[jtrain2$train==0])

#3
jtrain2 %>% filter(train == 1, unem78 == 1) %>% nrow() / jtrain2 %>% filter(train == 1) %>% nrow()
jtrain2 %>% filter(train == 0, unem78 == 1) %>% nrow() / jtrain2 %>% filter(train == 0) %>% nrow()
```

### C5

```{r 1.C5}
data("fertil2")

#1
range(fertil2$children)

#2
nrow(fertil2[fertil2$electric == 1, ]) / nrow(fertil2)

#3
mean(fertil2$children[fertil2$electric == 1], na.rm = TRUE)
mean(fertil2$children[fertil2$electric == 0], na.rm = TRUE)
```

# Chapter 2: The Simple Regression Model
## Exampeles
### 2.3

```{r}
data("ceosal1")

summ(lm(salary ~ roe, data = ceosal1))

ggplot(ceosal1, aes(roe, salary)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```


### 2.4

```{r, message=FALSE, warning=FALSE}
summ(lm(log(wage) ~ educ, data = wage1))

ggplot(wage1, aes(educ, log(wage))) + 
  geom_point() +
  geom_smooth(method = "lm")
```

### 2.11

```{r 2.E2.11}
summ(lm(log(salary) ~ log(sales), data = ceosal1))

ggplot(ceosal1, aes(log(sales), log(salary))) + 
  geom_point() +
  geom_smooth(method = "lm ")
```

## Exercises
### C1

```{r 2.C1}
data("k401k")
head(k401k)

#1
mean(k401k$prate)
mean(k401k$mrate)

#2
summ(lm(prate ~ mrate, data = k401k))
```

### C2

```{r 2.C2}
data("ceosal2")

#1
mean(ceosal2$salary)
mean(ceosal2$ceoten)

#2
length(ceosal2$ceoten[ceosal2$ceoten == 0])
max(ceosal2$ceoten)

#3
summ(lm(log(salary) ~ ceoten, data = ceosal2))
```

### C3

```{r 2.C3}
data("sleep75")

#1
summ(lm(sleep ~ totwrk, data = sleep75))

#2
lm(sleep ~ totwrk, data = sleep75)$coefficient[2] * 2
```

### C4

```{r 2.C4}
data("wage2")

#1
mean(wage2$wage)
mean(wage2$IQ)

#2
summ(lm(wage ~ log(IQ), data = wage2))
lm(wage ~ log(IQ), data = wage2)$coefficients[2] * log(15)

#3
summ(lm(log(wage) ~ log(IQ), data = wage2))
lm(log(wage) ~ log(IQ), data = wage2)$coefficients[2] * log(15)
```

### C5

```{r 2.C5}
data("rdchem")

#1
# log(sales) = beta0 + beta1 * log(rd) + u

#2
summ(lm(log(sales) ~ log(rd), data = rdchem))
```

### C6

```{r 2.C6}
data("meap93")

#1
# Diminishing effect

#2
summ(lm(log(math10) ~ log(expend), data = meap93))
```

### C7

```{r 2.C7}
data("charity")

#1
mean(charity$gift)
length(charity$gift[charity$gift == 0]) / length(charity$gift) * 100

#2
mean(charity$mailsyear)
range(charity$mailsyear)

#3
summ(lm(gift ~ mailsyear, data = charity))
```

### C8

```{r 2.C8}
#1
X <- runif(10000, 0, 10)
mean(X)
sd(X)

#2
u <- rnorm(10000, 0, 6)
mean(u)
sd(u)

#3
Y <- 1 + 2 * X + u
summ(lm(Y ~ X))

#4
round(sum(lm(Y ~ X)$residuals), 3)
round(sum(X * lm(Y ~ X)$residuals), 3)

#5
round(sum(u), 3)
round(sum(X * u), 3)
```

# Chapter 3: Multiple Regression Analysis: Estimation
## Examples
### 3.1

```{r 3.E3.1}
data("gpa1")

summ(lm(colGPA ~ hsGPA + ACT, data = gpa1))
```

### 3.2

```{r 3.E3.2}
summ(lm(log(wage) ~ educ + exper + tenure, data = wage1))
```

### 3.3

```{r 3.E3.3}
summ(lm(prate ~ mrate + age, data = k401k))
summ(lm(prate ~ mrate, data = k401k))

cor(k401k$mrate, k401k$age)
```

### 3.5

```{r 3.E3.5}
data("crime1")

summ(lm(narr86 ~ pcnv + avgsen + ptime86 + qemp86, data = crime1))
```

## Exercises
### C1

```{r 3.C1}
#1
# Negative for beta2 (cigs)

#2
# Negatively correlated
cor(bwght$cigs, bwght$faminc)

#3
summ(lm(bwght ~ cigs, data = bwght))
summ(lm(bwght ~ cigs + faminc, data = bwght))
```

### C2

```{r 3.C2}
data("hprice1")

#1
summ(lm(price ~ sqrft + bdrms, data = hprice1))
# price = -19.315 + 0.128 * sqrft + 15.198 * bdrms + u

#2
# 15.198 * 1000

#3
# (140 * 0.128 + 1 * 15.198) * 1000 = 33118

#4
# R^2 = 0.632

#5
# -19.315 + 0.128 * 2438 + 15.198 * 4 = 353.541

#6
# 353.541 - 300 = 53.541
```

### C3

```{r 3.C3}
data("ceosal2")

#1
summ(lm(log(salary) ~ log(sales) + log(mktval), data = ceosal2))
# log(salary ) = 4.621 + 0.162 * log(sales) + 0.107 * log(mktval) + u

#2
summ(lm(log(salary) ~ log(sales) + log(mktval) + profits, data = ceosal2))

#3
summ(lm(log(salary) ~ log(sales) + log(mktval) + profits + ceoten, 
        data = ceosal2))
# 1.2%

#4
cor(ceosal2$lmktval, ceosal2$profits)
# High correlation leads to positive biases.
```

### C4

```{r 3.C4}
data("attend")

#1
summary(attend$atndrte)
summary(attend$priGPA)
summary(attend$ACT)

#2
summ(lm(atndrte ~ priGPA + ACT, data = attend))
# atndrte = 75.700 + 17.261 * priGPA - 1.717 * ACT + u

#3
# estimate of ACT is surprising.

#4
75.700 + 17.261 * 3.65 - 1.717 * 20
# Unrealistic since it exceeds the maximum 100.

#5
17.261 * 3.1 - 1.717 * 21 - (17.261 * 2.1 - 1.717 * 26)
```

### C5

```{r 3.C5}
# Regress educ on exper and tenure
summ(lm(educ ~ exper + tenure, data = wage1))

# Save the residuals
r <- lm(educ ~ exper + tenure, data = wage1)$residuals

# Regress log(wage) on r
summ(lm(log(wage) ~ r, data = wage1))
summ(lm(log(wage) ~ educ + exper + tenure, data = wage1))
# Equal
```

### C6

```{r 3.C6}
#1
summ(lm(IQ ~ educ, data = wage2))
theta <- 3.534

#2
summ(lm(log(wage) ~ educ, data = wage2))
beta <-  0.060

#3
summ(lm(log(wage) ~ educ + IQ, data = wage2))
beta1 <- 0.039
beta2 <- 0.006

#4
round(beta1 + beta2 *theta, 3) == beta
```

### C7

```{r 3.C7}
#1
summ(lm(math10 ~ log(expend) + lnchprg, data = meap93))

#3
summ(lm(math10 ~ log(expend), data = meap93))

#4
cor(meap93$lexpend, meap93$lnchprg)

```

### C8

```{r 3.C8}
data("discrim")

#1
mean(discrim$prpblck, na.rm = TRUE)
sd(discrim$prpblck, na.rm = TRUE)
mean(discrim$income, na.rm = TRUE)
sd(discrim$income, na.rm = TRUE)
# Percentage
# Dollar

#2
summ(lm(psoda ~ prpblck + income, data = discrim))
# Sample size: 401
# R-squared: 0.064
# Estimate for prpblck: 0.115

#3
summ(lm(psoda ~ prpblck, data = discrim))
# Estimate for prpblck: 0.065
# Smaller

#4
summ(lm(log(psoda) ~ prpblck + log(income), data = discrim))
0.122 * 20

#5
summ(lm(log(psoda) ~ prpblck + log(income) + prppov, data = discrim))

#6
cor((discrim %>% drop_na(lincome, prppov))$lincome, 
    (discrim %>% drop_na(lincome, prppov))$prppov)
```

### C9

```{r 3.C9}
#1
summ(lm(gift ~ mailsyear + giftlast + propresp, data = charity))
# R-squared: 0.083
summ(lm(gift ~ mailsyear, data = charity))
# R-squared: 0.014

#2
# Smaller

#4
summ(lm(gift ~ mailsyear + giftlast + propresp + avggift, data = charity))
# Smaller
```

### C10

```{r 3.C10}
data("htv")

#1
range(htv$educ)
nrow(filter(htv, educ == 12)) / nrow(htv) * 100
mean(htv$educ)
mean(htv$fatheduc)
mean(htv$motheduc)

#2
summ(lm(educ ~ motheduc + fatheduc, data = htv))
# R-squared: 0.249

#3
summ(lm(educ ~ motheduc + fatheduc + abil, data = htv))

#4
htv$abil2 <- htv$abil^2
summ(lm(educ ~ motheduc + fatheduc + abil + abil2, data = htv))

#5
#Make new dataframe
motheduc <- mean(htv$motheduc)
motheduc_original <- htv$motheduc
fatheduc_original <- htv$fatheduc
fatheduc <- mean(htv$fatheduc)
abil <- htv$abil
abil2 <- htv$abil2

htv_predict <- data.frame(motheduc, fatheduc, motheduc_original, fatheduc_original, abil, abil2)

#Make prediction
htv_predict$predicted <- predict(lm(educ ~ motheduc + fatheduc + abil + abil2, data = htv),
                                 new = htv_predict)

#Make a point/line graph of fitted values
ggplot(htv_predict, aes(abil, predicted)) + 
  geom_point() + 
  geom_line()

#Make a 3d scatterplot of fitted values
scatterplot3d(htv_predict$abil, 
              (htv_predict$fatheduc_original + htv_predict$motheduc_original) / 2, 
              htv_predict$predicted, 
              angle = 60, color = "dodgerblue", pch = 1, 
              ylab = "abil", 
              xlab = "parents' education", 
              zlab = "education years")
```

# Chapter 4: Multiple Regression Analysis: Inference
## Examples
### 4.1

```{r 4.E4.1}
summ(lm(log(wage) ~ educ + exper + tenure, data = wage1))
```

### 4.2

```{r 4.E4.2}
summ(lm(math10 ~ totcomp + staff + enroll, data = meap93))
```

### 4.4

```{r 4.E4.4}
summ(lm(log(crime) ~ log(enroll), data = campus))
```

If H0 = 1, then t = (1.27 - 1)/.11 = `r round((1.27 - 1)/.11, 3)`.  
The one-sided 5% critical value for a _t_ distribution with 95 _df_ is about `r round(qt(0.95, df = 95), 3)`. Therefore, we clearly reject beta1 = 1 in favor of beta1 > 1 at the 5% level.

### 4.5

```{r}
#Model 1
model1 <- lm(log(salary) ~ years + gamesyr + bavg + hrunsyr + rbisyr, data = mlb1)
summary(model1)

#SSR
sse1 <- sum((fitted(model1) - mean(log(mlb1$salary)))^2)
ssr1 <- sum((fitted(model1) - log(mlb1$salary))^2)
sst1 <- sse1 + ssr1

#Model 2
model2 <- lm(log(salary) ~ years + gamesyr, data = mlb1)
summary(model2)

sse2 <- sum((fitted(model2) - mean(log(mlb1$salary)))^2)
ssr2 <- sum((fitted(model2) - log(mlb1$salary))^2)
sst2 <- sse2 + ssr2

#F-statistics
linearHypothesis(model1, c("bavg=0", "hrunsyr=0", "rbisyr=0"))
```

### 4.9

```{r}
model <- lm(bwght ~ cigs + parity + faminc + motheduc + fatheduc, data = bwght)
summary(model)

linearHypothesis(model, c("motheduc=0", "fatheduc=0"))
```

### 4.10

```{r}
model <- lm(log(price) ~ log(assess) + log(lotsize) + log(sqrft) + bdrms, data = hprice1)
summary(model)

linearHypothesis(model, c("log(assess)=1", "log(lotsize)=0", "log(sqrft)=0", "bdrms=0"))
```

## Exercises
### C1

```{r}
#1
#While holding log(expendB) and prtystrA controlled, 1% increase of expendA lead to 1*beta1% increase of voteA

#2
#H0: beta1 + beta2 = 0

#3
model <- lm(voteA ~ log(expendA) + log(expendB) + prtystrA, data = vote1)
summary(model)

#4
model <- lm(voteA ~ log(expendA-expendB) + prtystrA, data = vote1)
summary(model)
```

### C3

```{r}
model <- lm(log(price) ~ sqrft + bdrms, data = hprice1)
summ(model)

#1
theta1 <- 150 * 0.000 + 1 * 0.029

#2
#log(price) = beta0 + beta1 * (x1 - 150 * x2) + theta1 * x2 + u

#3
hprice1$add <- hprice1$sqrft - 150 * hprice1$bdrms
summ(lm(log(price) ~ add + bdrms, data = hprice1))
confint(lm(log(price) ~ add + bdrms, data = hprice1))
```

### C5

```{r}
#2
model <- lm(log(salary) ~ years + gamesyr + bavg + hrunsyr + rbisyr + runsyr + fldperc + sbasesyr, data = mlb1)
summ(model)

#3
linearHypothesis(model, c("bavg=0", "fldperc=0", "sbasesyr=0"))
```

### C6

```{r}
#1
model <- lm(log(wage) ~ educ + exper + tenure, data = wage2)
#H0: beta2 - beta3 = 0

#2
wage2$add <- wage2$exper + wage2$tenure
model <- lm(log(wage) ~ educ + add + tenure, data = wage2)
summ(model)
confint(model)
#We fail the reject H0 that they have the same effect
```

### C8

```{r}
#1
k401ksubs %>% filter(fsize == 1) %>% nrow()

#2
model <- lm(nettfa ~ inc + age, k401ksubs %>% filter(fsize == 1))
summ(model)

#4
t <- (0.843 - 1) / 0.092
pt(t, df = 2014)

#5
summ(lm(nettfa ~ inc, k401ksubs %>% filter(fsize == 1)))
```

### C9

```{r}
model <- lm(log(psoda) ~ prpblck + log(income) + prppov, discrim)
summ(model)

#1
# Beta1is statistically significantly different from zero at the 5% level, but not at the 1% level.

#3
model <- lm(log(psoda) ~ prpblck + log(income) + prppov + log(hseval), discrim)
summ(model)

#4
linearHypothesis(model, c("log(income)=0", "prppov=0"))
# Yes, jointly significant
```

### C11

```{r}
#1
model <- lm(educ ~ motheduc + fatheduc + abil + abil2, htv)
summ(model)

linearHypothesis(model, c("abil2=0"))

#3
model <- lm(educ ~ motheduc + fatheduc + abil + abil2 + tuit17 + tuit18, htv)
summ(model)

linearHypothesis(model, c("tuit17=0", "tuit18=0"))

#4
cor(htv$tuit17, htv$tuit18)
model <- lm(educ ~ motheduc + fatheduc + abil + abil2 + I((tuit17 + tuit18) / 2), htv)
summ(model)
```

# Chapter 5: Multiple Regression Analysis: OLS Asymptotics
## Examples
### 5.3

Lagrange Multiplier (_LM_) Test

```{r}
#1 Unrestricted and restricted model
model_ur <- lm(narr86 ~ pcnv + avgsen + tottime + ptime86 + qemp86, data = crime1)
model_r <- lm(narr86 ~ pcnv + ptime86 + qemp86, data = crime1)
#Save the residuals of the restricted model
model_r_residuals <- model_r$residuals

#2 Regress residuals on all independent variables
model_lm <- lm(model_r_residuals ~ pcnv + avgsen + tottime + ptime86 + qemp86, data = crime1)
r_squared <- summary(model_lm)$r.squared
lm_r <- r_squared * nobs(model_r)

#3
#Critical value of chi-squared distribution
qchisq(.95, df=2)
(p_value <- 1 - pchisq(lm_r, df = 2))
```

## Excercises
### C1

```{r}
#1
model <- lm(wage ~ educ + exper + tenure, data = wage1)

#2
model_log <- lm(log(wage) ~ educ + exper + tenure, data = wage1)

#3
hist(wage1$lwage)
#Log-level satifies Assumption MLR.6 more.
```

### C2

```{r}
#1
model <- lm(colgpa ~ hsperc + sat, data = gpa2)
summary(model)

#2
model2 <- lm(colgpa ~ hsperc + sat, data = gpa2[1:2070, ])
summary(model2)

#3
5.495e-04 / 7.185e-04
```

### C3

```{r}
#1
model_r <- lm(bwght ~ cigs + parity + faminc, data = bwght)
model_r_residuals <- model_r$residuals

model_lm <- lm(model_r_residuals ~ cigs + parity + faminc + motheduc + fatheduc, data = bwght)
r_squared <- summary(model_lm)$r.squared
lm_r <- r_squared * nobs(model_r)

(p_value <- 1 - pchisq(lm_r, df = model_r$df.residual))
```

### C4

```{r}
#1
skewness(filter(k401ksubs, fsize==1)$inc)
skewness(log(filter(k401ksubs, fsize==1)$inc))

#2
skewness(bwght2$bwght)
skewness(bwght2$lbwght)
```

### C5

```{r}
ggplot(htv, aes(x = educ)) + 
  geom_histogram(aes(y = ..density..)) + 
  stat_function(fun = dnorm, args = list(mean = mean(htv$educ), sd = sd(htv$educ)))
```

# Chapter 6: Multiple Regression Analysis: Further Issues
## Examples
### 6.1

```{r}
model <- lm(scale(price) ~ scale(nox) + scale(crime) + scale(rooms) + scale(dist) + scale(stratio), data = hprice2)
summ(model)
```

### 6.2

```{r}
model <- lm(log(price) ~ log(nox) + log(dist) + rooms + I(rooms^2) + stratio, data = hprice2)
summ(model)
0.545 / (2 * 0.062)
#turning point = 0.545 / (2 * 0.062)
```

### 6.3

```{r}
model <- lm(stndfnl ~ atndrte + priGPA + ACT + I(priGPA^2) + I(ACT^2) + priGPA * atndrte, 
            data = attend)
summ(model)
linearHypothesis(model, c("atndrte=0", "atndrte:priGPA=0"))

model <- lm(stndfnl ~ atndrte + priGPA + ACT + I(priGPA^2) + I(ACT^2) + I((priGPA - 2.59) * atndrte), 
            data = attend)
summ(model)

model <- lm(stndfnl ~ atndrte + priGPA + ACT + I((priGPA - 2.59)^2) + I(ACT^2) + I(priGPA * (atndrte-82)), 
            data = attend)
summ(model)
```

### 6.5

```{r}
model <- lm(colgpa ~ sat + hsperc + hsize + I(hsize^2), data = gpa2)
# sat = 1200, hsperc = 30, hsize = 5
model <- lm(colgpa ~ I(sat-1200) + I(hsperc-30) + I(hsize-5), data = gpa2) 
summary(model)
confint(model)
```

### 6.7

```{r}
#1 Obtain fitted values and residuals
model <- lm(log(salary) ~ log(sales) + log(mktval) + ceoten, ceosal2)
summary(model)

#2 Obtain alpha from equation 6.43
alpha <- 1 / nobs(model) * sum(exp(model$residuals))

#3 Obtain log(y) from given values of x1-k
y <- exp(4.504 + 0.163 * log(5000) + 0.109 * log(10000) + 0.0117 * 10)

#4 Obtain the predicted y
alpha * y
```

## Exercises
### C1

```{r}
#1
model <- lm(log(price) ~ log(dist), data = kielmc)
summ(model)

#2
model <- lm(log(price) ~ log(dist) + log(intst) + log(area) + log(land) + rooms + baths + age, 
            data = kielmc)
summ(model)

#3
model <- lm(log(price) ~ log(dist) + log(intst) + log(area) + log(land) + rooms + baths + age + I(log(intst)^2), 
            data = kielmc)
summ(model)
```

### C2

```{r}
#1
model <- lm(log(wage) ~ educ + exper + I(exper^2),
            data = wage1)
summ(model)

#3
100 * (0.041 - 2 * 0.0007 * 5) * 1
100 * (0.041 - 2 * 0.0007 * 20) * 1

#4
0.041 / (2 * 0.0007)
nrow(filter(wage1, exper > 29))
```

### C3

```{r}
#3
model <- lm(log(wage) ~ educ + exper + educ * exper, data = wage2)
summ(model)

#4
model <- lm(log(wage) ~ educ + exper + I(educ * exper - 10 * educ), data = wage2)
summ(model)
confint(model) %>% round(4)
```

### C4

```{r}
#1
model <- lm(sat ~ hsize + I(hsize^2), gpa2)
summ(model)

#2
19.8145 / (2 * 2.1306)

#4
model <- lm(log(sat) ~ hsize + I(hsize^2), gpa2)
summ(model)
0.0196 / (2 * 0.0021)
```

### C5

```{r}
#1
model <- lm(log(price) ~ log(lotsize) + log(sqrft) + bdrms, hprice1)
summ(model)

#2
-1.2970 + 0.1680 * log(20000) + 0.7002 * log(2500) + 0.00370 * 4
alpha <- 1 / nobs(model) * sum(exp(model$residuals))
alpha * exp(-1.2970 + 0.1680 * log(20000) + 0.7002 * log(2500) + 0.00370 * 4)

#3
model <- lm(price ~ lotsize + sqrft + bdrms, hprice1)
summ(model)
```

### C6

```{r}
#2
model <- lm(voteA ~ prtystrA + expendA + expendB + expendA * expendB, vote1)
summ(model)

#3
mean(vote1$expendA)
-0.0317 * 100000 - -0.0000 * 300

#4
0.0383 * 100 - 0.0000 * 100

#5
#No, it doesn't make sense.
```

### C7

```{r}
#1
model <- lm(stndfnl ~ atndrte + priGPA + ACT + I(priGPA^2) + I(ACT^2) + priGPA * atndrte, 
            data = attend)
summ(model)
-1.6285 + 2 * 0.2959 * 2.59 + 0.0056 * 82

#2
model <- lm(stndfnl ~ atndrte + priGPA + ACT + I((priGPA - 2.59)^2) + I(ACT^2) + I(priGPA * (atndrte-82)), 
            data = attend)
summ(model)
```

### C8

```{r}
#1
model <- lm(price ~ lotsize + sqrft + bdrms, hprice1)
summary(model)
new_data <- data.frame("lotsize"= 10000, "sqrft"=2300, bdrms=4)
predict(model, newdata = new_data) %>% round(0)

#2
model <- lm(price ~ I(lotsize-10000) + I(sqrft-2300) + I(bdrms-4), hprice1)
confint(model) %>% round(1)
```

### C9

```{r}
#1
model <- lm(points ~ exper + age + coll + I(exper^2), nbasal)
summ(model)

#2
2.3636 / (2 * 0.0770)

#4
model <- lm(points ~ exper + age + coll + I(exper^2) + I(age^2), nbasal)
summ(model)
3.9837 / (2 * 0.0536)

#5
model <- lm(log(wage) ~ points + exper + I(exper^2) + age + coll, nbasal)
summ(model)

#6
linearHypothesis(model, c("age=0", "coll=0"))
```

### C10

```{r}
#1
model <- lm(log(bwght) ~ npvis + I(npvis^2), bwght2)
summ(model)

#2
0.0189 / (2 * 0.0004)
nrow(filter(bwght2, npvis >=22))

#4
model <- lm(log(bwght) ~ mage + I(mage^2) + npvis + I(npvis^2), bwght2)
summ(model)
nrow(filter(bwght2, mage > 0.0254 / (2 * .0004)))

#5
#no

#6
model <- lm(bwght ~ mage + I(mage^2) + npvis + I(npvis^2), bwght2)
summ(model)
```

### C11

```{r}
#1
model <- lm(ecolbs ~ ecoprc + regprc, apple)
summ(model)

#3
range(fitted.values(model))
nrow(filter(apple, ecolbs == 0)) / nrow(apple)

#4
#no

#5
model <- lm(ecolbs ~ ecoprc + regprc + faminc + hhsize + educ + age, apple)
linearHypothesis(model, c("faminc=0", "hhsize=0", "educ=0", "age=0"))

#6
model <- lm(ecolbs ~ ecoprc, apple)
summ(model)
model <- lm(ecolbs ~ regprc, apple)
summ(model)
cor(apple$ecoprc, apple$regprc)
```

### C12

```{r}
#1
filter(k401ksubs, fsize == 1)$age %>% range()
nrow(filter(k401ksubs, fsize == 1) %>% filter(age == 25))

#2
model <- lm(nettfa ~ inc + age + I(age^2), filter(k401ksubs, fsize == 1))
summ(model)
1.3218 / (2 * 0.0256)

#4
model <- lm(nettfa ~ inc + age + I((age-25)^2), filter(k401ksubs, fsize == 1))
summ(model)

#5
model <- lm(nettfa ~ inc + I(age^2), filter(k401ksubs, fsize == 1))
summ(model)

#6
new_data <- data.frame('inc' = 30,
                       age = seq(25, 74, 1))
new_data$nettfa_fitted <- predict(model, newdata = new_data)
ggplot(new_data, aes(age, nettfa_fitted)) + 
  geom_point() +
  geom_line()
```

### C13

```{r}
#1
model <- lm(math4 ~ lexppp + lenroll + lunch, meap00_01)
summ(model)

#2
range(fitted.values(model))
range(meap00_01$math4)

#3
max(model$residuals)

#4
model <- lm(math4 ~ lexppp + lenroll + lunch + I(lexppp^2) + I(lenroll^2) + I(lunch^2), meap00_01)
summ(model)

#5
model <- lm(scale(math4) ~ scale(lexppp) + scale(lenroll) + scale(lunch), meap00_01)
summ(model)
```

### C14

```{r}
#1
model <- lm(lavgsal ~ bs, benefits)
summ(model)
1 - pt((-0.5035 + 1) / 0.1662, df = 1846)

#2
benefits$lbs <- log(benefits$bs)
range(benefits$bs)
range(benefits$lbs)
sd(benefits$bs)
sd(benefits$lbs)

#3
model <- lm(lavgsal ~ lbs, benefits)
summ(model)

#4
model <- lm(lavgsal ~ bs + lenroll + lstaff + lunch, benefits)
summ(model)

#5
model <- lm(lavgsal ~ bs + lenroll + lstaff + lunch + I(lunch^2), benefits)
summ(model)
```

